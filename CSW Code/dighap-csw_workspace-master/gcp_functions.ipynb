{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data to Google Cloud Storage\n",
    "\n",
    "The code below is based off of the code sample in this link: \n",
    "[code sample](https://cloud.google.com/storage/docs/uploading-objects#prereq-code-samples)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all of the needed Python packages for authenticating access to\n",
    "Google Cloud Platform and for interacting with Google Cloud Storage and Google\n",
    "BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from google.cloud import storage, bigquery\n",
    "from google.oauth2.service_account import Credentials\n",
    "import pydata_google_auth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to authenticate using `pydata_google_auth`. Click\n",
    "on the link and login using your Bayer email. Copy the token and post it into\n",
    "the prompt to finish authenticating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=262006177488-3425ks60hkk80fssi9vpohv88g6q1iqd.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform&state=0j9083d6Y2MZI4UQo6gdeyHRERczYp&prompt=consent&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "credentials = pydata_google_auth.get_user_credentials(\n",
    "    ['https://www.googleapis.com/auth/cloud-platform'],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below implements the steps for uploading a file to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://cloud.google.com/storage/docs/uploading-objects#prereq-code-samples\n",
    "def upload_file_to_gcs(\n",
    "    project: str, bucket_name: str, source_file_name: str,\n",
    "    destination_blob_name: str, credentials: Credentials,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Uploads a local file to Google Cloud Storage (GCS).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project : str\n",
    "        Name of Google Cloud project.\n",
    "    bucket_name : str\n",
    "        Name of bucket in GCS.\n",
    "    source_file_name : str\n",
    "        Name of local source file being uploaded.\n",
    "    destination_blob : str\n",
    "        Name of blob (path) where local file will be written in GCS.\n",
    "    credentials : Credentials\n",
    "        Credentials for Google Cloud from `pydata_google_auth`.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client(\n",
    "        project=project, credentials=credentials\n",
    "    )\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get a list of the files we want to upload to Google Cloud Storage,\n",
    "iterate through them and call the `upload_file_to_gcs` function, supplying\n",
    "the desired project and bucket names, the destination blob/file name, and\n",
    "the credentials we obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File example_data/example_data2.json uploaded to csw_workspace/example_data/example_data2.json.\n",
      "File example_data/example_data1.json uploaded to csw_workspace/example_data/example_data1.json.\n"
     ]
    }
   ],
   "source": [
    "json_files = glob(\"example_data/*.json\")\n",
    "for json_file in json_files:\n",
    "    upload_file_to_gcs(\n",
    "        \"bcs-genomics-analytics-sbx\",  # project\n",
    "        \"bcs-genomics-analytics-sbx-sandbox\",  # bucket\n",
    "        json_file,  # local file\n",
    "        f\"csw_workspace/{json_file}\",  # destination file\n",
    "        credentials\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Table and Load Data in Google Big Query\n",
    "\n",
    "The example code below is based on code from these doc pages: \n",
    "[link](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to define the table schema, which follows the same\n",
    "layout as the JSON files we uploaded in the previous section. The schema is a Python\n",
    "list of `bigquery.SchemaField` objects. The `chromosome` and `position` columns are \n",
    "`NULLABLE` entries. The `markers` field is a nested entry, so we need to include\n",
    "that it is a `RECORD` type that can be `REPEATED`. The `fields` argument is where\n",
    "we define the nested fields for `markers`. We only have two here, `genotype` and\n",
    "`count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema = [\n",
    "    bigquery.SchemaField(\n",
    "        \"chromosome\", \"INTEGER\", \"NULLABLE\",\n",
    "        description=\"Chromosome number.\"\n",
    "    ),\n",
    "    bigquery.SchemaField(\n",
    "        \"position\", \"FLOAT\", \"NULLABLE\",\n",
    "        description=\"0.1 cM bin position.\"\n",
    "    ),\n",
    "    bigquery.SchemaField(\n",
    "        \"markers\", \"RECORD\", \"REPEATED\",\n",
    "        description=\"Observed markers in the given 0.1 cM bin.\",\n",
    "        fields=[\n",
    "            bigquery.SchemaField(\n",
    "                \"genotype\", \"STRING\", \"NULLABLE\",\n",
    "                description=\"Observed genotype (e.g., AA, AT).\"\n",
    "            ),\n",
    "            bigquery.SchemaField(\n",
    "                \"count\", \"INTEGER\", \"NULLABLE\",\n",
    "                description=\"Number of times genotype was observed.\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the project name, create a BigQuery client, and give a name to the\n",
    "table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"bcs-genomics-analytics-sbx\"\n",
    "client = bigquery.Client(credentials=credentials, project=project_name)\n",
    "table_id = f\"{project_name}.gmlfs.csw_workspace_example_data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we create a table object with the `table_id` and schema we defined. We can also add clustering fields and a table description here. Then, we create the table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = bigquery.Table(table_id, schema=table_schema)\n",
    "table.clustering_fields = [\"chromosome\"]\n",
    "table.description = \"Toy data set for CSW Workspace.\"\n",
    "table = client.create_table(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to do is to load the data that we put in GCS into the table. We\n",
    "first set up some configuration to define our schema and to define the format of\n",
    "the input data file. Then we specify the path/URI to our example JSON data (this\n",
    "can use wildcards). Then, we use the client object we created before to call the\n",
    "`load_table_from_uri` function to actually load the data into the table. Finally,\n",
    "we wait for the upload to finish and then print the number of rows created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 rows.\n"
     ]
    }
   ],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=table_schema,\n",
    "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    ")\n",
    "uri = \"gs://bcs-genomics-analytics-sbx-sandbox/csw_workspace/example_data/*.json\"\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri,\n",
    "    table_id,\n",
    "    location=\"US\",  # Must match the destination dataset location.\n",
    "    job_config=job_config,\n",
    ")\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
